# üí´ Awesome-Token-Merge-for-MLLMs

 [![Awesome](https://awesome.re/badge.svg)](https://awesome.re) ![GitHub stars](https://img.shields.io/github/stars/JinXins/Awesome-Token-Merge-for-MLLMs?color=green) ![GitHub forks](https://img.shields.io/github/forks/JinXins/Awesome-Token-Merge-for-MLLMs?color=yellow&label=Fork)
 
## Welcome to Awesome-Token-Merge-for-MLLMs.  
*(If you know some related papers which don't conclute in this list, please tell me in **Issues**, Thk !)* ü•∞

If this repository has been helpful to you, please consider giving it a ‚≠êÔ∏è to show your support. Your support helps us reach more researchers and contributes to the growth of this resource. Thank you! 

<p align="center">
<img src="https://github.com/user-attachments/assets/419181d4-091e-48dd-8ecd-4b2b02899dd2" width=50% height=50% 
class="center">
</p>

## üìú Introduction

**We summarize awesome token merge / reduce / resample methods in vision model for multi-modal large language models.**

The list of awesome mixup augmentation methods is summarized in chronological order and is on updating. The main branch is modified according to [Awesome-Token-Compress](https://github.com/daixiangzi/Awesome-Token-Compress).

## üìñ Related Papers

* **Visual Instruction Tuning** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2304.08485-red?logo=arxiv" height="14" />  
*Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee*<br>
NIPS'2023 *(oral)* [[Paper](https://arxiv.org/abs/2304.08485)]
[[Code](https://github.com/haotian-liu/LLaVA)]
   <details close>
   <summary>LLaVA Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/3c291594-94bd-493e-8619-b017a2eadc8a" /></p>
   </details>

* **MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.02991-red?logo=arxiv" height="14" />  
*Jianjian Cao, Peng Ye, Shengze Li, Chong Yu, Yansong Tang, Jiwen Lu, Tao Chen*<br>
CVPR'2024 [[Paper](https://arxiv.org/abs/2403.02991)]
[[Code](https://github.com/double125/MADTP)]
   <details close>
   <summary>MADTP Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/b66f90e9-970a-4b2a-8142-32288bcee6f7" /></p>
   </details>

* **Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.03003-red?logo=arxiv" height="14" />  
*Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, Rongrong Ji*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2403.03003)]
   <details close>
   <summary>LLaVA-HR Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/c6578071-fbc1-4957-b5bb-70c2149cac89" /></p>
   </details>

* **TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.04473-red?logo=arxiv" height="14" />  
*Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, Xiang Bai*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2403.04473)]
[[Code](https://github.com/Yuliang-Liu/Monkey)]
   <details close>
   <summary>TextMonkey Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/a8c79ad7-051e-43c7-b558-391d825a3888" /></p>
   </details>

* **An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.06764-red?logo=arxiv" height="14" />  
*Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, Baobao Chang*<br>
ECCV'2024 *(oral)* [[Paper](https://arxiv.org/abs/2403.06764)]
[[Code](https://github.com/pkunlp-icler/FastV?tab=readme-ov-file)]
   <details close>
   <summary>FastV Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/9de6cc9b-0298-4f53-a166-7d94a397f40a" /></p>
   </details>

* **Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.09333-red?logo=arxiv" height="14" />  
*Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, Jinqiao Wang*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2403.09333)]
[[Code](https://github.com/jefferyZhan/Griffon)]
   <details close>
   <summary>Griffon V2 Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/e689f86c-ee3e-4155-bcae-26ed25dc371c" /></p>
   </details>

* **LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.15388-red?logo=arxiv" height="14" />  
*Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2403.15388)]
[[Code](https://github.com/42Shawn/LLaVA-PruMerge)]
   <details close>
   <summary>LLaVA-PruMerge Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/9ef6ad98-5edf-480b-83c1-4217e2632c25" /></p>
   </details>

* **DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2405.20985-red?logo=arxiv" height="14" />  
*Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, Lu Hou*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2405.20985)]
[[Code](https://github.com/yaolinli/DeCo)]
   <details close>
   <summary>DeCo Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/c15ef777-1af3-4e07-adaa-bb16781869ae" /></p>
   </details>

* **Efficient Large Multi-modal Models via Visual Context Compression** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2406.20092-red?logo=arxiv" height="14" />  
*Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, Alan Yuille*<br>
NIPS'2024 [[Paper](https://arxiv.org/abs/2406.20092)]
[[Code](https://github.com/Beckschen/LLaVolta)]
   <details close>
   <summary>LLaVolta Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/a80f95ad-0f6e-495c-9c25-2e46d2375c96" /></p>
   </details>

* **VoCo-LLaMA: Towards Vision Compression with Large Language Models** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2406.12275-red?logo=arxiv" height="14" />  
*Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Ying Shan, Yansong Tang*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2406.12275)]
[[Code](https://github.com/Yxxxb/VoCo-LLaMA)]
   <details close>
   <summary>VoCo-LLaMA Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/d2d772fe-f9de-4234-8aa3-530654688d38" /></p>
   </details>

* **TokenPacker: Efficient Visual Projector for Multimodal LLM** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2407.02392-red?logo=arxiv" height="14" />  
*Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, Lei Zhang*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2407.02392)]
[[Code](https://github.com/CircleRadon/TokenPacker)]
   <details close>
   <summary>TokenPacker Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/d9b39280-a617-4578-9ae1-7045cfd0e828" /></p>
   </details>

* **Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2407.14439-red?logo=arxiv" height="14" />  
*Renshan Zhang, Yibo Lyu, Rui Shao, Gongwei Chen, Weili Guan, Liqiang Nie*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2407.14439)]
[[Code](https://github.com/JiuTian-VL/TokenCorrCompressor)]
   <details close>
   <summary>Token-level Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/18b88c11-da91-42aa-bec4-7219d8d3de99" /></p>
   </details>

* **HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2408.10945-red?logo=arxiv" height="14" />  
*Kazi Hasan Ibn Arif, JinYi Yoon, Dimitrios S. Nikolopoulos, Hans Vandierendonck, Deepu John, Bo Ji*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2408.10945)]
   <details close>
   <summary>HiRED Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/7c760e17-5c45-4d9e-a6a0-9224027d79e3" /></p>
   </details>

* **Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.01179-red?logo=arxiv" height="14" />  
*Yi Chen, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2409.01179)]
   <details close>
   <summary>Recoverable Compression Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/06f8c5bb-fccc-46f8-ba75-a936e3a73a9c" /></p>
   </details>

* **TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.09564-red?logo=arxiv" height="14" />  
*Dawei Yan, Pengcheng Li, Yang Li, Hao Chen, Qingguo Chen, Weihua Luo, Wei Dong, Qingsen Yan, Haokui Zhang, Chunhua Shen*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2409.09564)]
   <details close>
   <summary>TG-LLaVA Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/0d4cbf41-95a9-408d-ae03-77ec64021692" /></p>
   </details>

* **Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.10994-red?logo=arxiv" height="14" />  
*Dingjie Song, Wenjun Wang, Shunian Chen, Xidong Wang, Michael Guan, Benyou Wang*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2409.10994)]
[[Code](https://github.com/FreedomIntelligence/TRIM/)]
   <details close>
   <summary>TRIM Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/17d96743-010d-4a77-932d-dc5989c6f9e7" /></p>
   </details>

* **AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.02745-red?logo=arxiv" height="14" />  
*Zhibin Lan, Liqiang Niu, Fandong Meng, Wenbo Li, Jie Zhou, Jinsong Su*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2410.02745)]
[[Code](https://github.com/DeepLearnXMU/AVG-LLaVA)]
   <details close>
   <summary>AVG-LLaVA Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/5624cff4-1565-442f-b8f3-6f29b4191171" /></p>
   </details>

* **Retrieval Replace Reduction: An effective visual token reduction method via semantic match** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.07278-red?logo=arxiv" height="14" />  
*Yingen Liu, Fan Wu, Ruihui Li, Zhuo Tang, Kenli Li*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2410.07278)]
   <details close>
   <summary>TRSM Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/4564afe1-7730-4e6a-94cd-2199332fb9dd" /></p>
   </details>

* **Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.14072-red?logo=arxiv" height="14" />  
*Yuxin Wen, Qingqing Cao, Qichen Fu, Sachin Mehta, Mahyar Najibi*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2410.14072)]
   <details close>
   <summary>Victor Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/e389c0ad-3e09-4b86-90f3-08e91d98905b" /></p>
   </details>

* **PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.17247-red?logo=arxiv" height="14" />  
*Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, Dahua Lin*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2410.17247)]
[[Code](https://github.com/Cooperx521/PyramidDrop)]
   <details close>
   <summary>PyramidDrop Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/7e70c47b-2b20-4719-b897-be2f92cc6b42" /></p>
   </details>

* **Inference Optimal VLMs Need Only One Visual Token but Larger Models** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.03312-red?logo=arxiv" height="14" />  
*Kevin Y. Li, Sachin Goyal, Joao D. Semedo, J. Zico Kolter*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2411.03312)]
[[Code](https://github.com/locuslab/llava-token-compression)]
   <details close>
   <summary>QuCC Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/abe6d820-c8cb-4eb4-97af-08874f3a5bd0" /></p>
   </details>

* **Don't Look Twice: Faster Video Transformers with Run-Length Tokenization** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.05222-red?logo=arxiv" height="14" />  
*Rohan Choudhury, Guanglei Zhu, Sihan Liu, Koichiro Niinuma, Kris M. Kitani, L√°szl√≥ Jeni*<br>
NIPS'2024 *(Spotlight)* [[Paper](https://arxiv.org/abs/2411.05222)]
[[Code](https://github.com/rccchoudhury/rlt)]
   <details close>
   <summary>RLT Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/262ff231-8523-42b1-a189-98a11aae4f02" /></p>
   </details>

* **Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.10803-red?logo=arxiv" height="14" />  
*Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, Linfeng Zhang*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2411.10803)]
[[Code](https://github.com/liuting20/MustDrop)]
   <details close>
   <summary>MustDrop Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/65afab06-12fc-40f3-86bd-f59f3f65c8fc" /></p>
   </details>

* **FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.14228-red?logo=arxiv" height="14" />  
*Yuke Zhu, Chi Xie, Shuang Liang, Bo Zheng, Sheng Guo*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2411.14228)]
   <details close>
   <summary>FocusLLaVA Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/1086d89b-3df3-4287-8042-8fda0daa69ff" /></p>
   </details>

<p align="right">(<a href="#top">back to top</a>)</p>
