# Awesome-Token-Merge-for-MLLMs

 [![Awesome](https://awesome.re/badge.svg)](https://awesome.re) ![GitHub stars](https://img.shields.io/github/stars/Westlake-AI/Awesome-Mixup?color=green) ![GitHub forks](https://img.shields.io/github/forks/Westlake-AI/Awesome-Mixup?color=yellow&label=Fork)
 
Welcome to Awesome-Token-Merge-for-MLLMs.
If this repository has been helpful to you, please consider giving it a ⭐️ to show your support. Your support helps us reach more researchers and contributes to the growth of this resource. Thank you! 

## Introduction

**We summarize awesome token merge / reduce / resample methods in vision model for multi-modal large language models.**

The list of awesome mixup augmentation methods is summarized in chronological order and is on updating. The main branch is modified according to [Awesome-Token-Compress](https://github.com/daixiangzi/Awesome-Token-Compress).

## Related Papers

* **Visual Instruction Tuning** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2304.08485-red?logo=arxiv" height="14" />  
*Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee*<br>
NIPS'2023 *(oral)* [[Paper](https://arxiv.org/abs/2304.08485)]
[[Code](https://github.com/haotian-liu/LLaVA)]
   <details close>
   <summary>LLaVA Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/3c291594-94bd-493e-8619-b017a2eadc8a" /></p>
   </details>

* **MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.02991-red?logo=arxiv" height="14" />  
*Jianjian Cao, Peng Ye, Shengze Li, Chong Yu, Yansong Tang, Jiwen Lu, Tao Chen*<br>
CVPR'2024 [[Paper](https://arxiv.org/abs/2403.02991)]
[[Code](https://github.com/double125/MADTP)]
   <details close>
   <summary>MADTP Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/b66f90e9-970a-4b2a-8142-32288bcee6f7" /></p>
   </details>

* **Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.03003-red?logo=arxiv" height="14" />  
*Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, Rongrong Ji*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2403.03003)]
   <details close>
   <summary>LLaVA-HR Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/c6578071-fbc1-4957-b5bb-70c2149cac89" /></p>
   </details>

* **TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.04473-red?logo=arxiv" height="14" />  
*Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, Xiang Bai*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2403.04473)]
[[Code](https://github.com/Yuliang-Liu/Monkey)]
   <details close>
   <summary>TextMonkey Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/a8c79ad7-051e-43c7-b558-391d825a3888" /></p>
   </details>

* **An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.06764-red?logo=arxiv" height="14" />  
*Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, Baobao Chang*<br>
ECCV'2024 *(oral)* [[Paper](https://arxiv.org/abs/2403.06764)]
[[Code](https://github.com/pkunlp-icler/FastV?tab=readme-ov-file)]
   <details close>
   <summary>FastV Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/9de6cc9b-0298-4f53-a166-7d94a397f40a" /></p>
   </details>

* **Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.09333-red?logo=arxiv" height="14" />  
*Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, Jinqiao Wang*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2403.09333)]
[[Code](https://github.com/jefferyZhan/Griffon)]
   <details close>
   <summary>Griffon V2 Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/e689f86c-ee3e-4155-bcae-26ed25dc371c" /></p>
   </details>

* **LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.15388-red?logo=arxiv" height="14" />  
*Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2403.15388)]
[[Code](https://github.com/42Shawn/LLaVA-PruMerge)]
   <details close>
   <summary>LLaVA-PruMerge Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/9ef6ad98-5edf-480b-83c1-4217e2632c25" /></p>
   </details>

* **DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2405.20985-red?logo=arxiv" height="14" />  
*Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, Lu Hou*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2405.20985)]
[[Code](https://github.com/yaolinli/DeCo)]
   <details close>
   <summary>DeCo Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/c15ef777-1af3-4e07-adaa-bb16781869ae" /></p>
   </details>

* **Efficient Large Multi-modal Models via Visual Context Compression** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2406.20092-red?logo=arxiv" height="14" />  
*Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, Alan Yuille*<br>
NIPS'2024 [[Paper](https://arxiv.org/abs/2406.20092)]
[[Code](https://github.com/Beckschen/LLaVolta)]
   <details close>
   <summary>LLaVolta Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/a80f95ad-0f6e-495c-9c25-2e46d2375c96" /></p>
   </details>

* **VoCo-LLaMA: Towards Vision Compression with Large Language Models** <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2406.12275-red?logo=arxiv" height="14" />  
*Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Ying Shan, Yansong Tang*<br>
arXiv'2024 [[Paper](https://arxiv.org/abs/2406.12275)]
[[Code](https://github.com/Yxxxb/VoCo-LLaMA)]
   <details close>
   <summary>VoCo-LLaMA Framework</summary>
   <p align="center"><img width="50%" src="https://github.com/user-attachments/assets/d2d772fe-f9de-4234-8aa3-530654688d38" /></p>
   </details>

